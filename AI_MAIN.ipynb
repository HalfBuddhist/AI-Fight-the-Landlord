{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基本设置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 变量编码\n",
    "\n",
    "### state编码：\n",
    "\n",
    "一维数组，大小是30，编号0-29\n",
    "\n",
    "- 编号0-14。表示当前玩家的手牌情况，0-12代表1-13的个数，13、14分别代表小王、大王的个数。\n",
    "- 编号15-29。表示当前所有玩家已出的手牌情况。具体编码同上。\n",
    "\n",
    "例子：\n",
    "\n",
    "玩家A的手牌是 [1 1 2 3]，玩家A,B,C已经出过的牌有[2,2,2,3,3,4]。这时候轮到A出牌了，A的state编码为：\n",
    "\n",
    "[2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] + [0, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "\n",
    "### action编码：\n",
    "\n",
    "一维数组，大小是403，编号0-402如下：\n",
    "\n",
    "- 0-14: 单出， 1-13，小王，大王\n",
    "- 15-27: 对，1-13\n",
    "- 28-40: 三，1-13\n",
    "- 41-196: 三带1，先遍历111.2，111.3，一直到131313.12\n",
    "- 197-352: 三带2，先遍历111.22,111.33,一直到131313.1212\n",
    "- 353-366: 炸弹，1111-13131313，加上王炸\n",
    "- 367-402: 先考虑5个的顺子，按照顺子开头从小到达进行编码，共计8+7+..+1=36\n",
    "- 403-415: 三带大王\n",
    "- 416-428: 三带小王\n",
    "    \n",
    "### reward编码：\n",
    "\n",
    "- 出完牌之后的state输的话，state为-1\n",
    "- 出完牌之后的state赢的话，state为1\n",
    "- 出完牌之后的state未结束的话，state为0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h = {}\n",
    "\n",
    "# 三带大王\n",
    "for i in range(403,416):\n",
    "    h[i] = str([i-402]*3 + [15])\n",
    "    \n",
    "# 三带小王\n",
    "for i in range(416,429):\n",
    "    h[i] = str([i-415]*3 + [14])\n",
    "\n",
    "# 单\n",
    "for i in range(15):\n",
    "    h[i] = str([i+1])\n",
    "assert i==14\n",
    "\n",
    "# 对\n",
    "for i in range(15,28):\n",
    "    h[i] = str([i-14]*2)\n",
    "assert i==27\n",
    "\n",
    "# 三\n",
    "for i in range(28,41):\n",
    "    h[i] = str([i-27]*3)\n",
    "assert i==40\n",
    "\n",
    "# 三带1\n",
    "l = []\n",
    "for m in range(13):\n",
    "    for n in range(13):\n",
    "        if m!=n:\n",
    "            curr = sorted([m+1]*3+[n+1])\n",
    "            l.append(str(curr))\n",
    "assert len(l)==13*12\n",
    "for i in range(41,197):\n",
    "    h[i] = l[i-41]\n",
    "assert i==196\n",
    "\n",
    "# 三带2\n",
    "l = []\n",
    "for m in range(13):\n",
    "    for n in range(13):\n",
    "        if m!=n:\n",
    "            curr = sorted([m+1]*3+[n+1]*2)\n",
    "            l.append(str(curr))\n",
    "assert len(l)==13*12\n",
    "for i in range(197,353):\n",
    "    h[i] = l[i-197]\n",
    "assert i==352\n",
    "\n",
    "# 炸弹\n",
    "353-366\n",
    "for i in range(353,367):\n",
    "    idx = i-352\n",
    "    if idx<=13:\n",
    "        h[i] = str([idx]*4)\n",
    "    else:\n",
    "        h[i] = str([14,15])\n",
    "assert i==366\n",
    "\n",
    "# 顺子\n",
    "l = []\n",
    "for size in range(8,0,-1):\n",
    "    length = 13 - size\n",
    "    curr = []\n",
    "    for i in map(lambda x: range(x,x+length), range(3,3+size)):\n",
    "        if i[-1]!=14:\n",
    "            curr.append(str(i))\n",
    "        else:\n",
    "            curr.append(str([1]+i[:-1]))\n",
    "    l = l + curr\n",
    "for i in range(367,403):\n",
    "    h[i] = l[i-367]\n",
    "assert i==402"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 验证\n",
    "assert len(h)==429"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 需要的~\n",
    "h_xugang = {h[i]:i for i in h}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 需要的API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ddz_rl.agent import Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q network\n",
    "\n",
    "## 输入\n",
    "\n",
    "- state_size大小: 30    13(1-4)+2(01值)\n",
    "- action大小：403（01所有可能情况:单(15)、对(13)、三(13)、三带1(13*12)、三带(13*12)、炸弹(13)、顺子5+(sum(range(2,11)))）\n",
    "- reward：当前的reward，最后的reward\n",
    "\n",
    "\n",
    "1. inputs_: [None, state_size]\n",
    "2. actions_: [None]\n",
    "3. targetQs_: [None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class QNetwork:\n",
    "    def __init__(self, learning_rate=0.01, state_size=30, \n",
    "                 action_size=429, hidden_size=20, \n",
    "                 name='QNetwork'):\n",
    "        # state inputs to the Q-network\n",
    "        with tf.variable_scope(name):\n",
    "            # state\n",
    "            self.inputs_ = tf.placeholder(tf.float32, [None, state_size], name='inputs')\n",
    "            # action\n",
    "            self.actions_ = tf.placeholder(tf.int32, [None], name='actions')\n",
    "            one_hot_actions = tf.one_hot(self.actions_, action_size)\n",
    "            \n",
    "            # Target Q values for training\n",
    "            self.targetQs_ = tf.placeholder(tf.float32, [None], name='target')\n",
    "            \n",
    "            # ReLU hidden layers\n",
    "            self.fc1 = tf.contrib.layers.fully_connected(self.inputs_, hidden_size, \n",
    "                activation_fn = tf.nn.relu, ) # [batch, hidden_size]\n",
    "            self.fc2 = tf.contrib.layers.fully_connected(self.fc1, hidden_size) # [batch, hidden_size]\n",
    "\n",
    "            # Linear output layer\n",
    "            self.output = tf.contrib.layers.fully_connected(self.fc2, action_size, \n",
    "                                                            activation_fn=None)\n",
    "            \n",
    "            ### Train with loss (targetQ - Q)^2\n",
    "            # Q value from Q-network\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, one_hot_actions), axis=1)\n",
    "            self.loss = tf.reduce_mean(tf.square(self.targetQs_ - self.Q))\n",
    "            self.opt = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_episodes = 1000          # max number of episodes to learn from\n",
    "max_steps = 200                # max steps in an episode\n",
    "gamma = 0.99                   # future reward discount\n",
    "\n",
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "hidden_size = 64               # number of units in each Q-network hidden layer\n",
    "learning_rate = 0.0001         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 10000            # memory capacity\n",
    "batch_size = 100                # experience mini-batch size\n",
    "pretrain_length = batch_size   # number experiences to pretrain the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "mainQN = QNetwork(name='main', hidden_size=hidden_size, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Populate the experience memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看这里需要什么\n",
    "\n",
    "1. 可选的action空间\n",
    "2. 选择action后获得state、reward、done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def random_action(agent):\n",
    "    action_space = agent.get_actions_space()\n",
    "    \n",
    "    if action_space:\n",
    "        yaobuqi = False\n",
    "        action_id = random.randint(0,len(action_space)-1)\n",
    "        action = action_space[action_id]\n",
    "#         try:\n",
    "        state, reward, done = agent.step(action_id=action_id)\n",
    "#         except:\n",
    "#             print action_space,action_id\n",
    "#             raise Exception(\"Wrong\")\n",
    "    else:\n",
    "        yaobuqi = True\n",
    "        state, reward, done = agent.step()\n",
    "    return state, reward, done, action, yaobuqi\n",
    "\n",
    "# 游戏开始，洗牌，发牌\n",
    "agent = Agent()\n",
    "state = agent.reset()\n",
    "\n",
    "# 初始化memory\n",
    "memory = Memory(max_size=memory_size)\n",
    "for ii in range(pretrain_length):\n",
    "\n",
    "    # 随机决策\n",
    "    next_state, reward, done, action, yaobuqi = random_action(agent)\n",
    "\n",
    "        \n",
    "    \n",
    "    # 要不起的情况下不添加训练数据集\n",
    "    if yaobuqi:\n",
    "        state = next_state\n",
    "        continue\n",
    "        \n",
    "    if done:\n",
    "        next_state = np.zeros(state.shape)\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        \n",
    "        # 重新洗牌，发牌\n",
    "        state = agent.reset()\n",
    "    else:\n",
    "        # 添加memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# traing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent = Agent()\n",
    "s = agent.reset()\n",
    "done = False\n",
    "while(not done):\n",
    "    #print agent.game.get_record().cards_left1\n",
    "    actions = agent.get_actions_space() #如果actions为[]，step()\n",
    "    #GY的RL程序\n",
    "    s_, r, done = agent.step(action_id=0)\n",
    "    #print agent.game.get_record().cards_left1\n",
    "    #print agent.game.get_record().cards_left2\n",
    "    #print agent.game.get_record().cards_left3\n",
    "    #print agent.game.get_record().records\n",
    "    #print \"====================\"        \n",
    "    #raw_input(\"\")\n",
    "    s = s_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_action(agent):\n",
    "    action_space = agent.get_actions_space()\n",
    "    \n",
    "    if action_space:\n",
    "        yaobuqi = False\n",
    "        action_id = random.randint(0,len(action_space)-1)\n",
    "        action = action_space[action_id]\n",
    "#         try:\n",
    "        state, reward, done = agent.step(action_id=action_id)\n",
    "#         except:\n",
    "#             print action_space,action_id\n",
    "#             raise Exception(\"Wrong\")\n",
    "    else:\n",
    "        yaobuqi = True\n",
    "        state, reward, done = agent.step()\n",
    "    return state, reward, done, action, yaobuqi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "done = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shape mismatch: value array of shape (2,) could not be broadcast to indexing result of shape (6,429)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d95f134edaca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# Set target_Qs to 0 for states where episode ends\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mepisode_ends\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mtarget_Qs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepisode_ends\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_Qs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shape mismatch: value array of shape (2,) could not be broadcast to indexing result of shape (6,429)"
     ]
    }
   ],
   "source": [
    "# Now train with experiences\n",
    "saver = tf.train.Saver()\n",
    "rewards_list = []\n",
    "\n",
    "agent.reset()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    step = 0\n",
    "    for ep in range(1, train_episodes):\n",
    "        total_reward = 0\n",
    "        t = 0\n",
    "        while not done: # ！ 这里改成当游戏还未结束\n",
    "            \n",
    "            # 指数衰减的随机探索\n",
    "            step += 1\n",
    "            explore_p = explore_stop + (explore_start - explore_stop)*np.exp(-decay_rate*step) \n",
    "            if explore_p > np.random.rand():\n",
    "                # 随机探索\n",
    "                state, reward, done, action, yaobuqi = random_action(agent)\n",
    "            else:\n",
    "                # 根据Q-network行动\n",
    "                feed = {mainQN.inputs_: state.reshape((1, np.product(state.shape)))}\n",
    "                Qs = sess.run(mainQN.output, feed_dict=feed)\n",
    "                print Qs\n",
    "                raise Exception\n",
    "                action = np.argmax(Qs)\n",
    "                next_state, reward, done, _ = (action)\n",
    "    \n",
    "            total_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                # the episode ends so no next state\n",
    "                next_state = np.zeros(state.shape)\n",
    "                t = max_steps\n",
    "                \n",
    "                print('Episode: {}'.format(ep),\n",
    "                      'Total reward: {}'.format(total_reward),\n",
    "                      'Training loss: {:.4f}'.format(loss),\n",
    "                      'Explore P: {:.4f}'.format(explore_p))\n",
    "                rewards_list.append((ep, total_reward))\n",
    "                \n",
    "                # Add experience to memory\n",
    "                memory.add((state, action, reward, next_state))\n",
    "                \n",
    "                # Start new episode\n",
    "                env.reset()\n",
    "                # Take one random step to get the pole and cart moving\n",
    "                state, reward, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "            else:\n",
    "                # Add experience to memory\n",
    "                memory.add((state, action, reward, next_state))\n",
    "                state = next_state\n",
    "                t += 1\n",
    "            \n",
    "            # Sample mini-batch from memory\n",
    "            batch = memory.sample(batch_size)\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            rewards = np.array([each[2] for each in batch])\n",
    "            next_states = np.array([each[3] for each in batch])\n",
    "            \n",
    "            # Train network\n",
    "            target_Qs = sess.run(mainQN.output, feed_dict={mainQN.inputs_: next_states})\n",
    "            \n",
    "            # Set target_Qs to 0 for states where episode ends\n",
    "            episode_ends = (next_states == np.zeros(states[0].shape)).all(axis=1)\n",
    "            target_Qs[episode_ends] = (0, 0)\n",
    "            \n",
    "            targets = rewards + gamma * np.max(target_Qs, axis=1)\n",
    "\n",
    "            loss, _ = sess.run([mainQN.loss, mainQN.opt],\n",
    "                                feed_dict={mainQN.inputs_: states,\n",
    "                                           mainQN.targetQs_: targets,\n",
    "                                           mainQN.actions_: actions})\n",
    "        \n",
    "    saver.save(sess, \"checkpoints/cartpole.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
