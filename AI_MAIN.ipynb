{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基本设置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 变量编码\n",
    "\n",
    "### state编码：\n",
    "\n",
    "一维数组，大小是30，编号0-29\n",
    "\n",
    "- 编号0-14。表示当前玩家的手牌情况，0-12代表1-13的个数，13、14分别代表小王、大王的个数。\n",
    "- 编号15-29。表示当前所有玩家已出的手牌情况。具体编码同上。\n",
    "\n",
    "例子：\n",
    "\n",
    "玩家A的手牌是 [1 1 2 3]，玩家A,B,C已经出过的牌有[2,2,2,3,3,4]。这时候轮到A出牌了，A的state编码为：\n",
    "\n",
    "[2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] + [0, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "\n",
    "### action编码：\n",
    "\n",
    "一维数组，大小是403，编号0-402如下：\n",
    "\n",
    "- 0-14: 单出， 1-13，小王，大王\n",
    "- 15-27: 对，1-13\n",
    "- 28-40: 三，1-13\n",
    "- 41-196: 三带1，先遍历111.2，111.3，一直到131313.12\n",
    "- 197-352: 三带2，先遍历111.22,111.33,一直到131313.1212\n",
    "- 353-366: 炸弹，1111-13131313，加上王炸\n",
    "- 367-402: 先考虑5个的顺子，按照顺子开头从小到达进行编码，共计8+7+..+1=36\n",
    "- 403-415: 三带大王\n",
    "- 416-428: 三带小王\n",
    "    \n",
    "### reward编码：\n",
    "\n",
    "- 出完牌之后的state输的话，state为-1\n",
    "- 出完牌之后的state赢的话，state为1\n",
    "- 出完牌之后的state未结束的话，state为0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h = {}\n",
    "\n",
    "# 三带大王\n",
    "for i in range(403,416):\n",
    "    h[i] = str([i-402]*3 + [15])\n",
    "    \n",
    "# 三带小王\n",
    "for i in range(416,429):\n",
    "    h[i] = str([i-415]*3 + [14])\n",
    "\n",
    "# 单\n",
    "for i in range(15):\n",
    "    h[i] = str([i+1])\n",
    "assert i==14\n",
    "\n",
    "# 对\n",
    "for i in range(15,28):\n",
    "    h[i] = str([i-14]*2)\n",
    "assert i==27\n",
    "\n",
    "# 三\n",
    "for i in range(28,41):\n",
    "    h[i] = str([i-27]*3)\n",
    "assert i==40\n",
    "\n",
    "# 三带1\n",
    "l = []\n",
    "for m in range(13):\n",
    "    for n in range(13):\n",
    "        if m!=n:\n",
    "            curr = sorted([m+1]*3+[n+1])\n",
    "            l.append(str(curr))\n",
    "assert len(l)==13*12\n",
    "for i in range(41,197):\n",
    "    h[i] = l[i-41]\n",
    "assert i==196\n",
    "\n",
    "# 三带2\n",
    "l = []\n",
    "for m in range(13):\n",
    "    for n in range(13):\n",
    "        if m!=n:\n",
    "            curr = sorted([m+1]*3+[n+1]*2)\n",
    "            l.append(str(curr))\n",
    "assert len(l)==13*12\n",
    "for i in range(197,353):\n",
    "    h[i] = l[i-197]\n",
    "assert i==352\n",
    "\n",
    "# 炸弹\n",
    "353-366\n",
    "for i in range(353,367):\n",
    "    idx = i-352\n",
    "    if idx<=13:\n",
    "        h[i] = str([idx]*4)\n",
    "    else:\n",
    "        h[i] = str([14,15])\n",
    "assert i==366\n",
    "\n",
    "# 顺子\n",
    "l = []\n",
    "for size in range(8,0,-1):\n",
    "    length = 13 - size\n",
    "    curr = []\n",
    "    for i in map(lambda x: range(x,x+length), range(3,3+size)):\n",
    "        if i[-1]!=14:\n",
    "            curr.append(str(i))\n",
    "        else:\n",
    "            curr.append(str([1]+i[:-1]))\n",
    "    l = l + curr\n",
    "for i in range(367,403):\n",
    "    h[i] = l[i-367]\n",
    "assert i==402"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 验证\n",
    "assert len(h)==429"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 需要的~\n",
    "h_xugang = {h[i]:i for i in h}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 需要的API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ddz_rl.agent import Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q network\n",
    "\n",
    "## 输入\n",
    "\n",
    "- state_size大小: 30    13(1-4)+2(01值)\n",
    "- action大小：403（01所有可能情况:单(15)、对(13)、三(13)、三带1(13*12)、三带(13*12)、炸弹(13)、顺子5+(sum(range(2,11)))）\n",
    "- reward：当前的reward，最后的reward\n",
    "\n",
    "\n",
    "1. inputs_: [None, state_size]\n",
    "2. actions_: [None]\n",
    "3. targetQs_: [None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class QNetwork:\n",
    "    def __init__(self, learning_rate=0.01, state_size=33, \n",
    "                 action_size=431, hidden_size=20, \n",
    "                 name='QNetwork'):\n",
    "        # state inputs to the Q-network\n",
    "        with tf.variable_scope(name):\n",
    "            # state\n",
    "            self.inputs_ = tf.placeholder(tf.float32, [None, state_size], name='inputs')\n",
    "            # action\n",
    "            self.actions_ = tf.placeholder(tf.int32, [None], name='actions')\n",
    "            one_hot_actions = tf.one_hot(self.actions_, action_size)\n",
    "            \n",
    "            # Target Q values for training\n",
    "            self.targetQs_ = tf.placeholder(tf.float32, [None], name='target')\n",
    "            \n",
    "            # ReLU hidden layers\n",
    "            self.fc1 = tf.contrib.layers.fully_connected(self.inputs_, hidden_size, \n",
    "                activation_fn = tf.nn.relu, ) # [batch, hidden_size]\n",
    "            self.fc2 = tf.contrib.layers.fully_connected(self.fc1, hidden_size) # [batch, hidden_size]\n",
    "\n",
    "            # Linear output layer\n",
    "            self.output = tf.contrib.layers.fully_connected(self.fc2, action_size, \n",
    "                                                            activation_fn=None)\n",
    "            \n",
    "            ### Train with loss (targetQ - Q)^2\n",
    "            # Q value from Q-network\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, one_hot_actions), axis=1)\n",
    "            self.loss = tf.reduce_mean(tf.square(self.targetQs_ - self.Q))\n",
    "            self.opt = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_episodes = 10000          # max number of episodes to learn from\n",
    "max_steps = 200                # max steps in an episode\n",
    "gamma = 0.99                   # future reward discount\n",
    "\n",
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "hidden_size = 64               # number of units in each Q-network hidden layer\n",
    "learning_rate = 0.0001         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 10000            # memory capacity\n",
    "batch_size = 100                # experience mini-batch size\n",
    "pretrain_length = batch_size   # number experiences to pretrain the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "mainQN = QNetwork(name='main', hidden_size=hidden_size, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Populate the experience memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看这里需要什么\n",
    "\n",
    "1. 可选的action空间\n",
    "2. 选择action后获得state、reward、done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# 游戏开始，洗牌，发牌\n",
    "agent = Agent()\n",
    "state = agent.reset()\n",
    "\n",
    "# 初始化memory\n",
    "memory = Memory(max_size=memory_size)\n",
    "for ii in range(pretrain_length):\n",
    "\n",
    "    # 随机决策\n",
    "    action_space = agent.get_actions_space()\n",
    "    action_id = random.randint(0,len(action_space)-1)\n",
    "    action = action_space[action_id]\n",
    "    next_state, reward, done = agent.step(action_id=action_id)        \n",
    "    \n",
    "    # 要不起的情况下不添加训练数据集\n",
    "    if action_id==430:\n",
    "        state = next_state\n",
    "        continue\n",
    "        \n",
    "    if done:\n",
    "        next_state = np.zeros(state.shape)\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        # 重新洗牌，发牌\n",
    "        state = agent.reset()\n",
    "    else:\n",
    "        # 添加memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# traing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Episode: 200', 'Success Rate: 0.27', 'Training loss: 748.1565', 'Explore P: 0.6516')\n",
      "('Episode: 400', 'Success Rate: 0.265', 'Training loss: 361.8442', 'Explore P: 0.4246')\n",
      "('Episode: 600', 'Success Rate: 0.12', 'Training loss: 10250.4365', 'Explore P: 0.2797')\n",
      "('Episode: 800', 'Success Rate: 0.11', 'Training loss: 90209.9922', 'Explore P: 0.1808')\n",
      "('Episode: 1000', 'Success Rate: 0.14', 'Training loss: 31659.7871', 'Explore P: 0.1181')\n",
      "('Episode: 1200', 'Success Rate: 0.19', 'Training loss: 10319.5908', 'Explore P: 0.0767')\n",
      "('Episode: 1400', 'Success Rate: 0.15', 'Training loss: 2509.6465', 'Explore P: 0.0517')\n",
      "('Episode: 1600', 'Success Rate: 0.05', 'Training loss: 5705.3438', 'Explore P: 0.0361')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-fd668af99bcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m                                 feed_dict={mainQN.inputs_: states,\n\u001b[1;32m     69\u001b[0m                                            \u001b[0mmainQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargetQs_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                                            mainQN.actions_: actions})\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# 当前游戏结束\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yangguangyao/anaconda/envs/p2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yangguangyao/anaconda/envs/p2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yangguangyao/anaconda/envs/p2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/yangguangyao/anaconda/envs/p2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yangguangyao/anaconda/envs/p2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "rewards_list = []\n",
    "records_list = []\n",
    "agent = Agent()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    step = 0\n",
    "    for ep in range(1, train_episodes):\n",
    "        \n",
    "        # 游戏初始化\n",
    "        agent.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        t = 0\n",
    "        \n",
    "        # 当前游戏进行\n",
    "        while not done:\n",
    "            # Action----------------------------------------\n",
    "            step += 1\n",
    "            explore_p = explore_stop + (explore_start - explore_stop)*np.exp(-decay_rate*step) \n",
    "            action_space = agent.get_actions_space()\n",
    "            if explore_p > np.random.rand():\n",
    "                # 随机探索\n",
    "                action_id = random.randint(0,len(action_space)-1)\n",
    "                action = action_space[action_id]\n",
    "            else:\n",
    "                # 根据Q-network行动\n",
    "                feed = {mainQN.inputs_: state.reshape((1, np.product(state.shape)))}\n",
    "                Qs = sess.run(mainQN.output, feed_dict=feed)\n",
    "                # 根据action space和q-network选择action\n",
    "                action_space_score = []\n",
    "                for i in action_space:\n",
    "                    action_space_score.append(Qs[0][i])\n",
    "                action_id = np.argmax(action_space_score)\n",
    "                action = action_space[action_id]\n",
    "\n",
    "            next_state, reward, done = agent.step(action_id=action_id)\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Memory----------------------------------------\n",
    "            if done:\n",
    "                next_state = np.zeros(state.shape)\n",
    "            memory.add((state, action, reward, next_state))\n",
    "            state = next_state\n",
    "            t += 1\n",
    "            \n",
    "            # Traing----------------------------------------\n",
    "            ## Sample mini-batch from memory\n",
    "            batch = memory.sample(batch_size)\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            rewards = np.array([each[2] for each in batch])\n",
    "            next_states = np.array([each[3] for each in batch])\n",
    "            \n",
    "            ## Train network\n",
    "            target_Qs = sess.run(mainQN.output, feed_dict={mainQN.inputs_: next_states})\n",
    "            \n",
    "            ## Set target_Qs to 0 for states where episode ends\n",
    "            episode_ends = (next_states == np.zeros(states[0].shape)).all(axis=1)\n",
    "            target_Qs[episode_ends] = [0]*431\n",
    "            \n",
    "            targets = rewards + gamma * np.max(target_Qs, axis=1)\n",
    "\n",
    "            loss, _ = sess.run([mainQN.loss, mainQN.opt],\n",
    "                                feed_dict={mainQN.inputs_: states,\n",
    "                                           mainQN.targetQs_: targets,\n",
    "                                           mainQN.actions_: actions})\n",
    "\n",
    "        # 当前游戏结束\n",
    "        # records_list.append(agent.game.get_record())\n",
    "        next_state = np.zeros(state.shape)\n",
    "        t = max_steps\n",
    "        rewards_list.append(total_reward)\n",
    "        if ep % 200==0 and ep>0:\n",
    "            print('Episode: {}'.format(ep),\n",
    "                  'Success Rate: {}'.format(np.sum(np.array(rewards_list)[-200:]==1)/200.0),\n",
    "                  'Training loss: {:.4f}'.format(loss),\n",
    "                  'Explore P: {:.4f}'.format(explore_p))\n",
    "        \n",
    "        \n",
    "    saver.save(sess, \"checkpoints/cartpole.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "records_list[170]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Qs = np.array([[3,4,1,5,7,6]])\n",
    "effect_ids = [0,5,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "effect_score = []\n",
    "for i in effect_ids:\n",
    "    effect_score.append(Qs[0][i])\n",
    "print effect_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = np.argmax(effect_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "action = effect_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.argmax(Qs)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
